{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 사용 예제"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch  2가지 주요 특징\n",
    " ### *Numpy와 유사하지만 GPU 상에서 실행 가능한 N차원 Tensor\n",
    " ### *신경망을 구성하고 학습하는 과정에서의 자동미분\n",
    " \n",
    " #### 완전히 연결된 ReLU 신경망을 예제로 사용\n",
    " #### 이 신경망은 하나의 은닉층(Hidden Layer)을 갖고 있으며, \n",
    " ##### 신경망의 출력과 정답 사이의 유클리드 거리(Euclidean distance)를 최소화하는 식으로 경사하강법(gradient descent)을 사용하여 무작위의 데이터를 맞추도록 학습할것이다\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " ReLU - > 활성화 함수의 한 종류 값이 0보다 작으면 0으로 0보다 크면 원래의 값을 줌\n",
    " \n",
    " \n",
    " ![image.png](img1.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단축키\n",
    "\n",
    "셀 추가하기\n",
    "\n",
    "- 현재 셀 위에 추가: (command 모드) a\n",
    "\n",
    "- 현재 셀 아래 추가: (command 모드) b\n",
    "\n",
    " \n",
    "\n",
    "셀 삭제하기\n",
    "\n",
    "- 현재 셀 삭제: (command 모드) dd\n",
    "\n",
    "- 셀 제거 취소하기: (command 모드) z\n",
    "\n",
    " \n",
    "\n",
    "셀 복사하기\n",
    "\n",
    "- 현재 셀 복사: (command 모드) c\n",
    "\n",
    " \n",
    "\n",
    "셀 붙여넣기\n",
    "\n",
    "- 현재 셀 위에 붙여 넣기: (command 모드) Shift + v\n",
    "\n",
    "- 현재 셀 아래 붙여 넣기: (command 모드) v\n",
    "\n",
    " \n",
    "\n",
    "셀 이동시키기\n",
    "\n",
    "- 위아래 버튼을 클릭해서 현재 셀을 이동시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27074569.324987695\n",
      "1 21373412.43936632\n",
      "2 19406215.79455047\n",
      "3 18233259.485912737\n",
      "4 16556206.744539391\n",
      "5 13892351.094379354\n",
      "6 10702579.857362375\n",
      "7 7601933.182357933\n",
      "8 5139242.142566733\n",
      "9 3391629.155378461\n",
      "10 2256784.6608108375\n",
      "11 1540655.3403623553\n",
      "12 1095386.4868471567\n",
      "13 813131.7266357783\n",
      "14 629313.3265511557\n",
      "15 504438.6316871742\n",
      "16 415873.71482615627\n",
      "17 350220.53055055963\n",
      "18 299681.3299189707\n",
      "19 259440.6244378205\n",
      "20 226586.26686111002\n",
      "21 199305.1725606317\n",
      "22 176287.21478756185\n",
      "23 156613.22457038774\n",
      "24 139632.10368428216\n",
      "25 124900.39999831485\n",
      "26 112046.20751538538\n",
      "27 100785.49045256028\n",
      "28 90881.60565202798\n",
      "29 82126.87241757632\n",
      "30 74364.50838875021\n",
      "31 67470.28911302339\n",
      "32 61323.34189127212\n",
      "33 55827.12770634581\n",
      "34 50900.53677435556\n",
      "35 46475.024147109325\n",
      "36 42493.73929720195\n",
      "37 38904.96670720927\n",
      "38 35663.64949242624\n",
      "39 32730.428109035853\n",
      "40 30071.62186482804\n",
      "41 27659.111727018404\n",
      "42 25466.11370049391\n",
      "43 23469.098638800533\n",
      "44 21648.674576079917\n",
      "45 19987.315968266063\n",
      "46 18469.23120632368\n",
      "47 17079.704140813872\n",
      "48 15808.681110209403\n",
      "49 14644.688130612958\n",
      "50 13576.211457347905\n",
      "51 12593.352188974266\n",
      "52 11689.141237961208\n",
      "53 10856.740102785989\n",
      "54 10089.462939231613\n",
      "55 9381.6266855251\n",
      "56 8728.23186475903\n",
      "57 8124.815868084221\n",
      "58 7567.190411957146\n",
      "59 7051.433539006084\n",
      "60 6573.869182614389\n",
      "61 6131.444200516902\n",
      "62 5721.517019399068\n",
      "63 5341.100450680372\n",
      "64 4988.020493578958\n",
      "65 4660.277745410276\n",
      "66 4355.949217242216\n",
      "67 4072.907769790271\n",
      "68 3809.5899732794355\n",
      "69 3564.4056005378015\n",
      "70 3336.144165361285\n",
      "71 3123.379326157611\n",
      "72 2925.1787544336844\n",
      "73 2740.6397476467178\n",
      "74 2568.507393412345\n",
      "75 2407.8028519036493\n",
      "76 2257.8450945439913\n",
      "77 2117.8293393843705\n",
      "78 1986.7797476603328\n",
      "79 1864.381086282406\n",
      "80 1749.9741281390357\n",
      "81 1643.0202800253996\n",
      "82 1542.958253084445\n",
      "83 1449.3102000611511\n",
      "84 1361.6626816780754\n",
      "85 1279.5943991880285\n",
      "86 1202.7351048004962\n",
      "87 1130.733932352451\n",
      "88 1063.28991618187\n",
      "89 1000.0992062505625\n",
      "90 940.869914048165\n",
      "91 885.3092040479158\n",
      "92 833.1777819538274\n",
      "93 784.2617163836792\n",
      "94 738.3862068248854\n",
      "95 695.3706428485277\n",
      "96 654.9623959338969\n",
      "97 617.0177125963801\n",
      "98 581.3748094827598\n",
      "99 547.8878671666469\n",
      "100 516.4147537562501\n",
      "101 486.8338418181388\n",
      "102 459.0329567771392\n",
      "103 432.8885937537185\n",
      "104 408.31400431865154\n",
      "105 385.17567672810645\n",
      "106 363.40553856173693\n",
      "107 342.9252149829251\n",
      "108 323.64174060889366\n",
      "109 305.4926773995086\n",
      "110 288.3988985528149\n",
      "111 272.31182429368255\n",
      "112 257.1568186731547\n",
      "113 242.88129004622522\n",
      "114 229.43057318203338\n",
      "115 216.75477439139965\n",
      "116 204.81308521094945\n",
      "117 193.54912170947776\n",
      "118 182.92905846646374\n",
      "119 172.9168085004402\n",
      "120 163.47705720092856\n",
      "121 154.57248601452358\n",
      "122 146.16889109649262\n",
      "123 138.2394861439487\n",
      "124 130.7582629121186\n",
      "125 123.69869846512017\n",
      "126 117.03087200832374\n",
      "127 110.73642509520275\n",
      "128 104.79602015847195\n",
      "129 99.18425621805022\n",
      "130 93.8845940340205\n",
      "131 88.8793757355873\n",
      "132 84.15362044977155\n",
      "133 79.69119428314346\n",
      "134 75.47275989092236\n",
      "135 71.48551739896635\n",
      "136 67.7174842011524\n",
      "137 64.1546618466233\n",
      "138 60.78605500953178\n",
      "139 57.60263679916669\n",
      "140 54.590606166858116\n",
      "141 51.74091618522261\n",
      "142 49.04715780440488\n",
      "143 46.498528610068654\n",
      "144 44.085934523207285\n",
      "145 41.80347618101476\n",
      "146 39.64315362066058\n",
      "147 37.598362199909516\n",
      "148 35.66308464758135\n",
      "149 33.831719391030916\n",
      "150 32.09744403243221\n",
      "151 30.454628152486954\n",
      "152 28.89915823094486\n",
      "153 27.425434953150383\n",
      "154 26.0292695703096\n",
      "155 24.706855908274445\n",
      "156 23.454774238596954\n",
      "157 22.267356080043445\n",
      "158 21.14238483632019\n",
      "159 20.07666432088795\n",
      "160 19.06575074924106\n",
      "161 18.107844811729546\n",
      "162 17.199647467120347\n",
      "163 16.338634163781027\n",
      "164 15.522137489161429\n",
      "165 14.74766045371284\n",
      "166 14.013305377393616\n",
      "167 13.316532726649807\n",
      "168 12.655546284731365\n",
      "169 12.028572955833795\n",
      "170 11.43389742705273\n",
      "171 10.86915985977755\n",
      "172 10.333362003111144\n",
      "173 9.824676842518693\n",
      "174 9.34212077828516\n",
      "175 8.883944078845227\n",
      "176 8.448974055689082\n",
      "177 8.036167721254873\n",
      "178 7.643961383839356\n",
      "179 7.271508699404774\n",
      "180 6.917777721446607\n",
      "181 6.581925230485647\n",
      "182 6.262781440749822\n",
      "183 5.959596454664906\n",
      "184 5.671472809175009\n",
      "185 5.397832170232456\n",
      "186 5.1377625273422405\n",
      "187 4.890699181132974\n",
      "188 4.655862857065644\n",
      "189 4.432620292237992\n",
      "190 4.220364313506831\n",
      "191 4.018592716766422\n",
      "192 3.8267322407949838\n",
      "193 3.644452274632348\n",
      "194 3.471050693634006\n",
      "195 3.3061222688319614\n",
      "196 3.149223941856242\n",
      "197 3.0000087645426747\n",
      "198 2.85814657404916\n",
      "199 2.7231523012208156\n",
      "200 2.594731742253172\n",
      "201 2.472579651743118\n",
      "202 2.3562875174054376\n",
      "203 2.245621888693756\n",
      "204 2.140313627128261\n",
      "205 2.0400894889689205\n",
      "206 1.9447131689039803\n",
      "207 1.8538927027055234\n",
      "208 1.767472739163507\n",
      "209 1.6851741643904992\n",
      "210 1.6068122974829158\n",
      "211 1.5322011367217774\n",
      "212 1.4611670646075359\n",
      "213 1.3935156796734944\n",
      "214 1.3290504550908802\n",
      "215 1.2676562223480237\n",
      "216 1.2091875180221452\n",
      "217 1.1535266214967559\n",
      "218 1.1004959083013313\n",
      "219 1.0499362573132107\n",
      "220 1.0017498217681042\n",
      "221 0.9558456669632184\n",
      "222 0.9121075012268314\n",
      "223 0.8704325607759844\n",
      "224 0.8307012640211653\n",
      "225 0.7928230083627412\n",
      "226 0.7567156569484862\n",
      "227 0.7223077744421884\n",
      "228 0.6895023855389846\n",
      "229 0.6582502168527138\n",
      "230 0.6284273536409835\n",
      "231 0.5999854693823405\n",
      "232 0.5728700688529651\n",
      "233 0.5470030534808838\n",
      "234 0.5223363247968423\n",
      "235 0.49881952945423536\n",
      "236 0.4763815547345776\n",
      "237 0.4549750993422672\n",
      "238 0.4345622628699952\n",
      "239 0.4150767866508082\n",
      "240 0.39649728237047044\n",
      "241 0.37877455133924887\n",
      "242 0.3618583665989535\n",
      "243 0.345706757723513\n",
      "244 0.3302979197766629\n",
      "245 0.3155880544014832\n",
      "246 0.30155230112428877\n",
      "247 0.2881561032728869\n",
      "248 0.2753739055778778\n",
      "249 0.26316452365083143\n",
      "250 0.25150984399572274\n",
      "251 0.24038628680342639\n",
      "252 0.2297631139417155\n",
      "253 0.21962065837201003\n",
      "254 0.20993640047151962\n",
      "255 0.20069022897096706\n",
      "256 0.19185797613346595\n",
      "257 0.18342109299676684\n",
      "258 0.17536331148979017\n",
      "259 0.16767395506337265\n",
      "260 0.16032605163815306\n",
      "261 0.15330645374844576\n",
      "262 0.14659780004505632\n",
      "263 0.1401924079671157\n",
      "264 0.13407034171164434\n",
      "265 0.12822108809683444\n",
      "266 0.12263514822622087\n",
      "267 0.11729825475950248\n",
      "268 0.11219522682768591\n",
      "269 0.10731953232853794\n",
      "270 0.10266125575508361\n",
      "271 0.0982098381710431\n",
      "272 0.09395468033135046\n",
      "273 0.08988568499299263\n",
      "274 0.08599727134955981\n",
      "275 0.0822808583832528\n",
      "276 0.07872682481604751\n",
      "277 0.0753296585789066\n",
      "278 0.07208311568731753\n",
      "279 0.06897808388124388\n",
      "280 0.0660104536010052\n",
      "281 0.06317253881931803\n",
      "282 0.060459200236523444\n",
      "283 0.05786404570659326\n",
      "284 0.05538256718150907\n",
      "285 0.05300960515667958\n",
      "286 0.05073936771786575\n",
      "287 0.04856815946684704\n",
      "288 0.04649150288691189\n",
      "289 0.04450557160605617\n",
      "290 0.04260647283303438\n",
      "291 0.040789888730124735\n",
      "292 0.03905178844089734\n",
      "293 0.037388523523285914\n",
      "294 0.035797486617308416\n",
      "295 0.034275768962052214\n",
      "296 0.032819612728242936\n",
      "297 0.031426130210692654\n",
      "298 0.03009333423893327\n",
      "299 0.028818613532463774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "\n",
    "# Gradient Descent를 실행하고 모델의 가중치를 갱신할때 고려할 데이터의 갯수다.\n",
    "# 배치 크기가 1이라면 Stochastic Gradient Descent가 된다. 훈련이 느리다.\n",
    "# 배치 크기가 데이터 전부의 갯수라면 Batch Mode가 된다. 계산량이 많다.\n",
    "# 배치 크기가 그 사이라면, Mini-Batch Mode가 되며, 좋은 GPU의 사용이 가능하다면 각각의 데이터마다 동시에 병렬 계산이 되기 때문에 제일 빠른 수렴이 가능하다.\n",
    "# 배치 크기는 GPU에 따라 64, 128, 256, 512.. 등으로 설정한다.\n",
    "# Iteration * Batch Size = Epoch 이므로, 한 데이터셋을 도는데 최대한 병렬처리가 가능하게끔 배치 크기를 주는게 이득이다.\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = np.random.randn(N, D_in)                 # 64행 * 1000열의 행렬 생성 후 randn -> 평균 : 0, 표준편차 : 1 의 가우시안분포를 가진 수로 초기화 \n",
    "y = np.random.randn(N, D_out)                # 64행 * 10열 행렬 생성 후 초기화  \n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in, H)                # 1000행 * 100열의 행렬 생성 후 초기화\n",
    "w2 = np.random.randn(H, D_out)               # 100행 * 10열의 행렬 생성 후 초기화\n",
    "\n",
    "\n",
    "learning_rate = 1e-6  #(0.000001)  # 아래 그림 참조          \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test1 =np.random.rand(3,5);\n",
    "test2 = np.random.rand(5,2);\n",
    "\n",
    ".dot => 행렬 곱 함수\n",
    "\n",
    "result= test1.dot(test2);\n",
    "print(result)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for t in range(300):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1)                                         # 입력데이터(64*1000) * 가중치w1(1000*100) 행렬 곱\n",
    "    h_relu = np.maximum(h, 0)                             # numpy.maximum()함수 : 두 배열 사이의 최대값인 배열을 반환 -> 0인 배열과 비교하였기 때문에 0보다 낮은 값들은 0으로 변환\n",
    "                                                          # (64,100) * (1000,100) -> shape = (64,100)\n",
    "    \n",
    "    y_pred = h_relu.dot(w2)                               # (64,100)*(100,10) -> shape = (64,10)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = np.square(y_pred - y).sum()                    \n",
    "    print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "\n",
    "![image.png](img2.png)\n",
    "\n",
    "### 경사 하강 그래프에서 최소점을 찾아가는 과정\n",
    "#### Learning rate는 너무 커도 안되며, 너무작아도 안된다.\n",
    "#### 너무 큰 경우, 최저값에 가까워지지않고 발산하는 현상이 발생 -> OverShooting\n",
    "#### 너무 작은 경우, 최저값에 가기도 전에 값을 확정하여 Local Minimum 현상이 발생 \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 활성화 함수\n",
    "\n",
    "![image.png](img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: Tensors\n",
    "NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없습니다. 현대의 심층 신경망에서 GPU는 종종 50배 또는 그 이상 의 속도 향상을 제공하기 때문에, 안타깝게도 NumPy는 현대의 딥러닝에는 충분치 않습니다.\n",
    "\n",
    "이번에는 PyTorch의 기본적인 개념인 Tensor 에 대해서 알아보겠습니다. PyTorch Tensor는 개념적으로 NumPy 배열과 동일합니다: Tensor는 N차원 배열이며, PyTorch는 Tensor 연산을 위한 다양한 함수들을 제공합니다. NumPy 배열처럼 PyTorch Tensor는 딥러닝이나 연산 그래프, 변화도는 알지 못하며, 과학적 분야의 연산을 위한 포괄적인 도구입니다.\n",
    "\n",
    "그러나 NumPy와는 달리, PyTorch Tensor는 GPU를 활용하여 수치 연산을 가속화할 수 있습니다. GPU에서 PyTorch Tensor를 실행하기 위해서는 단지 새로운 자료형으로 변환(Cast)해주기만 하면 됩니다.\n",
    "\n",
    "여기에서는 PyTorch Tensor를 사용하여 2계층의 신경망이 무작위 데이터를 맞추도록 할 것입니다. 위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계는 직접 구현하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1134,  0.0053, -1.3002,  0.0826, -1.9503]])\n",
      "99 번째 학습결과 :  1.4912989954041223e-08\n",
      "199 번째 학습결과 :  4.642643602892349e-09\n",
      "299 번째 학습결과 :  3.94222743338446e-09\n",
      "399 번째 학습결과 :  2.498417028817812e-09\n",
      "499 번째 학습결과 :  2.407467558640519e-09\n",
      "tensor([[ 0.1135,  0.0054, -1.3002,  0.0827, -1.9503]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 1,1000,100,5\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)   \n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)   \n",
    "print(y)\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t,\"번째 학습결과 : \", loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    \n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x25fd5aadac0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 3060 Ti'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N,D_in,H,D_out = 64 ,1000,100,10 \n",
    "\n",
    "# N = 배치 크기\n",
    "# D_in = 입력의 차원\\\n",
    "# H = 은닉층 차원\n",
    "# D_out = 출력 차원\n",
    "\n",
    "\n",
    "#평균 0, 표준편차 1 가우시안 분포를 가진 수로 초기화\n",
    "x =torch.randn(N,D_in,device=device,dtype=dtype)           #(64*1000)\n",
    "y =torch.randn(N,D_out,device=device,dtype=dtype)          #(64*10)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device = device , dtype=dtype)   #(1000*100)\n",
    "w2 = torch.randn(H, D_out, device = device , dtype=dtype)  #(100*10) \n",
    "\n",
    "learning_rate = 1e-6\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h= x.mm(w1)      #  mm() matrix multiple  = x * w1  matmul()과 동일\n",
    "# h = (64*1000) * (1000*100) => (64*100)\n",
    "h_relu = h.clamp(min=0)  # Relu 구현 .clamp() 메소드는 괄호파라미터에 맞춰 값을 변경 0이하의 값 0으로 다시 초기화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "y_pred = h_relu.mm(w2) # h_relu 값에 가중치 w2 를 곱하여 y의 예측값 생성\n",
    "\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29188380.0\n"
     ]
    }
   ],
   "source": [
    "loss =(y_pred -y).pow(2).sum().item() # pow() 제곱 함수\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.0048e+02, -1.7479e+02, -5.8992e+01,  1.5619e+02,  5.9190e+02,\n",
      "         -3.6686e+02,  1.2100e+02, -2.9196e+02, -6.9209e+02,  6.4368e+01],\n",
      "        [ 5.8196e+02, -2.1709e+02,  2.4207e+02,  6.6474e+02,  3.3918e+02,\n",
      "          1.2477e+02,  1.0560e+02,  3.6996e+01, -7.7050e+01,  4.8015e+02],\n",
      "        [ 4.5271e+02, -1.7733e+02,  7.6102e+02,  2.3675e+02,  3.7323e+02,\n",
      "          4.0880e+02, -2.9005e+00,  3.9681e+02, -4.6470e+02,  8.2836e+02],\n",
      "        [ 3.7340e+02, -5.6754e+02, -2.6155e+02,  6.7381e+02, -3.8972e+01,\n",
      "         -6.5597e+01,  8.3393e+02,  6.8324e+02, -2.0800e+02, -1.5339e+02],\n",
      "        [ 1.8038e+02, -3.3309e+01,  4.2103e+02,  8.0236e+02,  3.6306e+02,\n",
      "          5.3085e+01,  3.2288e+02,  1.1710e+02, -1.1485e+02,  4.0780e+02],\n",
      "        [ 8.0426e+02, -1.1290e+02, -1.9770e+02,  7.9558e+02, -9.0394e+01,\n",
      "          7.4667e+02, -2.1051e+01,  1.3872e+02, -3.1446e+02,  4.9768e+02],\n",
      "        [ 4.2447e+02, -8.5218e+01, -2.3473e+02,  7.3914e+02,  5.0670e+02,\n",
      "          9.7288e+02,  1.6804e+02,  1.2437e+02, -2.6806e+02, -3.2403e+02],\n",
      "        [ 4.6322e+02,  5.3859e+02,  5.4538e+02,  5.0579e+02,  2.9798e+02,\n",
      "         -4.5333e+02, -9.0286e+01,  5.8183e+02, -3.0030e+02,  7.2223e+01],\n",
      "        [ 7.5469e+02,  2.3242e+01,  4.7381e+02,  1.2526e+02,  8.5978e+02,\n",
      "         -1.6954e+02, -4.7246e+02,  2.6717e+02, -6.2456e+02,  6.0506e+02],\n",
      "        [ 4.5201e+02, -3.8017e+02,  4.6479e+02,  1.3944e+02,  8.5349e+01,\n",
      "         -3.8166e+02, -1.2541e+02,  1.2818e+02,  1.3737e+02,  3.2347e+02],\n",
      "        [ 4.9437e+02, -1.7557e+02,  2.3982e+02,  5.8760e+02, -7.5290e+01,\n",
      "          4.7010e+02,  4.6548e+02,  5.3426e+02, -4.1263e+02, -4.1835e+01],\n",
      "        [ 3.3051e+02, -2.1686e+02,  2.6262e+02,  3.3350e+02, -3.4217e+02,\n",
      "         -2.7179e+02, -5.0413e+02,  8.0154e+02, -1.2739e+01,  1.5880e+02],\n",
      "        [ 1.9920e+02,  2.5390e+02,  4.8829e+02,  1.3481e+02,  4.1415e+02,\n",
      "         -1.9492e+02,  9.3970e+00, -1.9885e+02, -4.7433e+02, -7.2964e+01],\n",
      "        [ 2.9878e+02, -6.2872e+02,  7.5684e+02,  5.0222e+02,  6.7565e+02,\n",
      "         -1.8826e+02,  6.3011e+01,  3.9269e+02, -3.7246e+02, -3.8931e+01],\n",
      "        [ 3.2516e+02, -4.0437e+02,  2.5369e+02, -1.0846e+02,  7.0209e+02,\n",
      "         -3.1226e+02,  3.6101e+02, -8.8942e+01, -6.4799e+01,  5.6996e+02],\n",
      "        [ 2.9972e+02, -3.4418e+02,  5.1891e+01,  1.4607e+02,  3.5676e+02,\n",
      "          1.1664e+02,  4.8066e+02,  3.7980e+02, -5.3057e+02,  5.9976e+01],\n",
      "        [ 6.2577e+02, -2.4068e+02,  2.1424e+02,  3.2837e+02,  3.9076e+02,\n",
      "         -2.6371e+01,  1.6198e+02, -2.8555e+02,  2.7910e+02,  5.2097e+02],\n",
      "        [-8.3459e+01, -1.3593e+01, -3.7889e+02,  1.3728e+02,  8.1176e+01,\n",
      "          3.6074e+02, -4.3573e+02,  5.2234e+02,  3.3061e+02, -2.6594e+02],\n",
      "        [ 8.8475e+02,  5.4018e+02,  2.4647e+02,  6.3248e+02,  3.1440e+02,\n",
      "          4.7363e+02, -2.5014e+01,  1.5061e+02, -5.2463e+02,  3.8728e+02],\n",
      "        [ 7.5564e+02, -3.0881e+02, -5.0994e+00,  7.8445e+02,  4.4168e+02,\n",
      "         -3.1623e+00,  4.3010e+02,  7.8440e+02, -6.3765e+02,  3.7690e+02],\n",
      "        [-1.9702e+02,  2.2139e+01,  3.8540e+02,  9.3754e+02,  2.8253e+02,\n",
      "          3.0217e+02, -2.5026e+02,  2.9710e+02, -6.0386e+02,  3.0096e+02],\n",
      "        [ 1.5243e+03,  3.3539e+02,  4.6434e+02,  8.8699e+02,  6.9084e+01,\n",
      "          5.1961e+02,  9.5067e+02,  7.9525e+02, -7.1126e+02,  2.7538e+02],\n",
      "        [ 4.3628e+02,  6.7124e+02,  8.3968e+02,  3.2669e+02,  5.5445e+02,\n",
      "          6.4328e+02, -3.5616e+02, -3.1198e+02, -6.3015e+02,  4.8505e+02],\n",
      "        [ 8.2167e+02,  1.7799e+02,  8.7744e+02,  5.2623e+02,  3.3023e+02,\n",
      "          4.8776e+02,  4.9932e+02, -1.6093e+02, -3.2869e+02,  8.7416e+00],\n",
      "        [ 2.8520e+02,  4.2837e+02,  6.9038e+01,  8.6668e+02, -2.6557e+02,\n",
      "         -6.4304e+02, -1.4381e+02,  7.4592e+02, -6.6399e+02,  4.6192e+00],\n",
      "        [-7.0467e+01, -3.8090e+02, -4.0478e+02, -4.9055e+01, -2.2446e+02,\n",
      "         -1.7289e+01,  9.1980e+02, -1.2798e+01,  2.9609e+02, -4.4847e+02],\n",
      "        [ 6.9201e+02, -2.3840e+02,  4.4153e+02,  4.4314e+02, -1.2172e+02,\n",
      "          3.6391e+01, -1.6776e+02,  2.1058e+02, -7.3115e+02,  8.7268e+02],\n",
      "        [ 1.0202e+03,  2.3932e+02, -1.5908e+02,  6.4787e+02, -1.4978e+01,\n",
      "          1.9579e+02,  4.5895e+02,  5.3669e+02, -2.1616e+02,  1.1908e+02],\n",
      "        [ 4.6533e+01, -2.5890e+01,  7.8945e+02,  2.3895e+02,  1.2945e+03,\n",
      "          9.2909e+01,  6.0722e+02, -4.0171e+02, -4.0886e+02,  1.8486e+02],\n",
      "        [-1.4730e+02, -2.0771e+02, -1.5248e+02,  1.6745e+02,  4.9471e+02,\n",
      "         -5.2629e+02, -2.0848e+01, -6.5872e+01, -4.7239e+02,  5.4058e+02],\n",
      "        [ 4.7154e+02, -1.4257e+02,  1.0967e+02,  2.8441e+02, -2.9082e+02,\n",
      "         -6.2433e+02,  1.8233e+02,  4.2248e+02,  2.5850e+01,  3.5743e+02],\n",
      "        [ 1.9300e+02, -1.6030e+02,  2.5202e+02,  5.8203e+02,  4.6498e+02,\n",
      "         -2.5680e+02, -6.4695e+01,  7.7789e+00, -4.5417e+02,  1.7008e+02],\n",
      "        [ 9.0174e+02,  3.5987e+01,  9.7540e+02,  1.5160e+02,  1.8945e+01,\n",
      "          4.2104e+02, -2.4492e+02,  3.5512e+02, -1.4044e+02,  2.5054e+01],\n",
      "        [ 5.3558e+02, -1.5363e+02,  4.2749e+02,  2.4251e+02,  7.9507e+01,\n",
      "         -1.0529e+02, -1.3143e+02,  5.9687e+02, -7.9166e+02,  8.3636e+02],\n",
      "        [ 2.6218e+02, -1.7235e+02,  6.5236e+02,  4.8068e+02, -1.1768e+01,\n",
      "          4.5419e+02,  3.2587e+02,  5.0558e+02, -1.4113e+02,  3.8142e+02],\n",
      "        [ 4.2050e+02, -6.5839e+02, -1.6865e+02,  3.2657e+02,  2.4940e+00,\n",
      "         -5.1676e+02,  1.8141e+02,  5.1795e+02,  9.8116e+00,  1.8151e+02],\n",
      "        [ 2.4458e+02, -9.4355e+00,  9.8781e+01,  3.5139e+02,  6.5296e+02,\n",
      "          7.0055e+02,  2.3357e+02, -4.1603e+02, -2.7498e+02, -2.4158e+02],\n",
      "        [ 3.3327e+02,  7.5937e+02, -2.3102e+01,  2.7582e+02,  2.5670e+02,\n",
      "          2.6686e+02, -3.2497e+02,  2.7835e+02, -1.2473e+03,  5.2123e+02],\n",
      "        [ 9.4278e+02, -1.8615e+01,  4.9588e+02,  4.3314e+02,  6.0471e+01,\n",
      "         -3.4749e+01,  1.9447e+02,  3.7053e+01, -4.7111e+02,  6.8169e+02],\n",
      "        [-2.3488e+02, -7.2903e+02, -6.1408e+02,  4.9617e+02,  5.6550e+01,\n",
      "          3.5194e+02,  4.4039e+02, -1.9787e+02, -2.8853e+02,  6.9024e+02],\n",
      "        [ 3.2300e+02, -4.7632e+02,  9.3313e+02,  4.4931e+02,  3.4761e+02,\n",
      "          2.6730e+02,  5.9904e+02,  5.9522e+01, -3.8216e+02,  5.4085e+01],\n",
      "        [-1.6617e+02, -3.9003e+02, -2.3863e+02,  2.8592e+01,  4.8577e+01,\n",
      "         -1.3589e+02, -3.0511e+02,  1.9813e+02, -8.7698e+01, -9.3686e+01],\n",
      "        [ 3.0710e+02, -3.0841e+02,  3.9597e+02,  5.4207e+02,  5.8057e+01,\n",
      "          2.3625e+02,  6.6918e+02,  1.3348e+02,  5.9206e+01, -1.7708e+01],\n",
      "        [ 3.1576e+02, -7.9412e+02,  1.1321e+02,  1.8953e+02,  4.0403e+02,\n",
      "         -1.1123e+02, -2.9625e+02,  6.7584e+02, -2.5298e+02,  5.8619e+02],\n",
      "        [ 1.3969e+02, -3.9230e+02, -3.9957e+02,  4.8070e+02,  1.6008e+02,\n",
      "          5.9507e+02,  9.2559e+01,  4.4983e+02,  4.6628e+01,  1.4747e+02],\n",
      "        [ 4.1334e+02, -5.4311e+02,  3.5755e+02,  6.1369e+02, -3.3490e+02,\n",
      "         -9.7985e+01,  1.4406e+02, -1.1268e+02, -2.2409e+02, -1.5152e+02],\n",
      "        [ 3.1780e+02,  5.5778e+01,  8.9083e+02,  3.2137e+02,  1.4688e+02,\n",
      "         -2.5347e+02, -1.3648e+02,  4.7410e+01, -9.2366e+02,  9.7385e+02],\n",
      "        [ 5.6042e+02,  4.8697e+01, -1.0713e+02,  3.5074e+02, -4.4580e+01,\n",
      "          9.5435e+01,  4.0923e+02,  2.0529e+02, -6.1608e+02, -1.8226e+02],\n",
      "        [ 6.4794e+02,  2.2815e+02,  2.2618e+01,  1.2513e+02, -1.5128e+01,\n",
      "         -7.6350e+01,  4.8287e+02,  4.0909e+02, -2.5580e+02,  8.4664e+02],\n",
      "        [ 8.9886e+02, -3.5679e+01,  2.6959e+02,  5.4318e+02, -5.7102e+02,\n",
      "          5.0504e+02,  1.3582e+02,  6.1776e+02, -5.2232e+01, -1.7736e+01],\n",
      "        [-1.3585e+01, -7.9063e+01,  3.3698e+02,  2.6308e+02,  3.6034e+02,\n",
      "         -2.4349e+01, -4.2220e+02,  4.2071e+02, -5.7014e+01,  1.6784e+02],\n",
      "        [ 3.8301e+02, -6.0212e+02,  3.3554e+02,  1.0111e+03, -1.6718e+01,\n",
      "         -4.1875e+02, -1.4617e+02,  4.7086e+02, -5.6322e+02, -4.3673e+02],\n",
      "        [ 3.1414e+02, -2.2443e+02,  3.7696e+02,  5.0235e+02,  2.0736e+02,\n",
      "         -8.9112e+02,  3.0501e+02,  4.6411e+02, -3.3340e+02, -1.5355e+02],\n",
      "        [ 1.0661e+03,  3.2651e+02, -3.3603e+02,  7.7666e+02,  2.1187e+02,\n",
      "          3.4437e+02,  3.5319e+02,  2.8428e+02, -8.1611e+02,  3.1189e+01],\n",
      "        [ 3.1371e+02, -3.9023e+02, -1.4545e+01,  4.8630e+02,  1.4158e+02,\n",
      "          3.4694e+01,  3.0815e+02,  1.2255e+02, -1.8511e+02,  2.0175e+02],\n",
      "        [ 2.5850e+02, -1.7861e+01,  6.0986e+01,  1.9392e+02,  6.8257e+01,\n",
      "          5.0519e+02, -8.4102e+00, -2.1906e+02, -4.4426e+02,  6.0800e+02],\n",
      "        [ 6.8652e+02, -8.8541e+01,  4.0284e+02,  8.9603e+01, -5.1661e+02,\n",
      "         -8.9674e+01,  2.8217e+02,  5.0189e+02, -5.6002e+02, -1.1673e+02],\n",
      "        [ 2.2510e+02, -3.2394e+02,  2.4940e+02,  2.4869e+02,  2.0854e+02,\n",
      "         -9.1368e+02, -1.9176e+02,  8.9370e+01, -2.2036e+02,  2.3468e+02],\n",
      "        [-1.4168e+02, -5.4106e+02, -6.2669e+01,  2.1134e+02,  1.4000e+02,\n",
      "         -3.4097e+02, -3.3775e+02,  7.3738e+02, -2.8227e+02,  1.0856e+02],\n",
      "        [ 4.5176e+02,  2.3081e+02,  4.5111e+02, -2.9713e+01,  4.1232e+02,\n",
      "         -1.9932e+02, -6.3240e+02,  4.7049e+02, -1.5069e+02,  4.4185e+02],\n",
      "        [ 3.8825e+02,  1.8622e+02,  1.7481e+02,  1.0303e+03,  4.0353e+02,\n",
      "         -1.6213e+02, -7.1582e+02, -1.6599e+02, -9.4569e+02, -1.4529e+02],\n",
      "        [ 5.8617e+02, -6.2619e+02,  3.1353e+02,  9.7640e+01, -2.7193e+02,\n",
      "          1.9676e+02, -3.6065e+02,  8.1707e+02, -3.1419e+02,  2.3687e+02],\n",
      "        [ 1.7120e+02, -4.9744e+02,  1.5343e+02,  7.6919e+02,  6.5250e+01,\n",
      "          3.6052e+01, -4.2606e+02,  8.5213e+02,  5.7662e+00, -1.3677e+01],\n",
      "        [ 5.2490e+02, -9.1074e+00, -4.7572e+01,  6.3141e+02,  1.6574e+02,\n",
      "          9.7197e-01, -3.3350e+02,  1.7958e+02, -3.5106e+02,  1.8135e+02]])\n"
     ]
    }
   ],
   "source": [
    "grad_y_pred = 2.0 * (y_pred - y)\n",
    "\n",
    "print(grad_y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w2 가중치 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n",
      "torch.Size([100, 64])\n"
     ]
    }
   ],
   "source": [
    "grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "\n",
    "# h_relu의 값을 전치행렬로 변환하여 예측한 y의값을 곱하여 w2 가중치 구현\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h layer의  가중치 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_h_relu = grad_y_pred.mm(w2.t())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_h = grad_h_relu.clone() \n",
    "grad_h[h < 0] = 0\n",
    "# - 처음의 h 부분에서 0 이하의 수가 있던 위치를 구하여 0으로 반환시켰다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w1의 가중치 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w1 = x.t().mm(grad_h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 -= learning_rate * grad_w1\n",
    "w2 -= learning_rate * grad_w2\n",
    "# 처음에 지정한 learning rate를 w1, w2의 가중치에 곱해준 뒤 w1, w2에서 제하였다.\n",
    "\n",
    "# 부호가 -인 이유는 loss를 감소시키는 방향으로 진행되어야 하기 때문이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2637, -1.3518, -0.4090,  ...,  1.3705, -0.9176, -2.4547],\n",
      "        [-0.1293, -1.1505, -0.4713,  ...,  0.7086,  0.4912, -0.0106],\n",
      "        [ 0.0377, -1.4680, -0.5666,  ...,  0.1435,  0.4030,  2.0523],\n",
      "        ...,\n",
      "        [ 0.1476, -1.1028,  0.9612,  ..., -1.1382,  0.0423,  1.4626],\n",
      "        [ 0.4726,  1.0323, -1.3779,  ...,  0.2149, -0.5539,  0.0235],\n",
      "        [ 0.4146,  0.3210,  1.0878,  ...,  0.0158, -0.3656,  0.5798]])\n",
      "tensor([[-1.2637, -1.3518, -0.4090,  ...,  1.3705, -0.9176, -2.4547],\n",
      "        [-0.1293, -1.1505, -0.4713,  ...,  0.7086,  0.4912, -0.0106],\n",
      "        [ 0.0377, -1.4680, -0.5666,  ...,  0.1435,  0.4030,  2.0523],\n",
      "        ...,\n",
      "        [ 0.1476, -1.1028,  0.9612,  ..., -1.1382,  0.0423,  1.4626],\n",
      "        [ 0.4726,  1.0323, -1.3779,  ...,  0.2149, -0.5539,  0.0235],\n",
      "        [ 0.4146,  0.3210,  1.0878,  ...,  0.0158, -0.3656,  0.5798]])\n"
     ]
    }
   ],
   "source": [
    "print(w1)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
